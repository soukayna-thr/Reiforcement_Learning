{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1332937e-f64a-4e05-bc84-54f29b549f6e",
   "metadata": {},
   "source": [
    "**Machine Learning II - TP1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94b2bfa4-7b43-411b-bbd6-fc46f86592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e5b84-a5a0-4012-94d3-77ebb04cef92",
   "metadata": {},
   "source": [
    "Exercice 1 : Exploration de l'environnement FrozenLake\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3016ad1-8211-4ef4-a8b7-d1692dc767b7",
   "metadata": {},
   "source": [
    "* **Chargement de l'environnement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f33a302-32d2-442e-9662-2c72988e7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c9c76c4-4426-490f-ab14-df105385c5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc1780-066c-40aa-861d-da68aec31764",
   "metadata": {},
   "source": [
    "* **Affichage des informations de l'espace d'états et d'actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98b58833-45a1-486f-b426-b73d3e41e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(4)\n",
      "Espace d'observations : Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45c872-c8aa-460b-a3f8-37434b390b3a",
   "metadata": {},
   "source": [
    "* **Les observations et les récompenses obtenus quand l'agent prend des actions aléatoires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7df52971-eec2-4c08-ae67-0f88f80d751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fc24ae4-848a-4e59-b587-50cc47f32218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode 1 :\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 8, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 8, Reward : 0.0\n",
      "Action : 3, Observation : 9, Reward : 0.0\n",
      "Action : 2, Observation : 10, Reward : 0.0\n",
      "Action : 1, Observation : 14, Reward : 0.0\n",
      "Action : 2, Observation : 10, Reward : 0.0\n",
      "Action : 2, Observation : 14, Reward : 0.0\n",
      "Action : 3, Observation : 15, Reward : 1.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "\n",
      "Épisode 2 :\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 8, Reward : 0.0\n",
      "Action : 1, Observation : 12, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "\n",
      "Épisode 3 :\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 3, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 7, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    print(f\"\\nÉpisode {episode + 1} :\")\n",
    "    state = env.reset()[0]\n",
    "    for _ in range(20):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "        print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "    \n",
    "        if done:\n",
    "            env.reset()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce0f20-d6e0-4392-90ab-ccf7669e8475",
   "metadata": {},
   "source": [
    "Exercice 2 : Implémentation de la Q-Table et Initialisation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a10e9-1910-4e30-9d89-bd42bc288270",
   "metadata": {},
   "source": [
    "* **Création  de la Q-Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd194ed8-43f5-457c-934d-4a396003d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed0f2b-ab39-4349-9332-7589a8c362cf",
   "metadata": {},
   "source": [
    "* **Affichage du table avant l'apprentissage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18ed9434-ebdf-44fe-9595-344f4e8bf577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table initiale :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nQ-Table initiale :\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ac548-4696-4636-a5af-d9dccd46eab3",
   "metadata": {},
   "source": [
    "* **Vérification que chaque état a une liste de valeurs pour chaque action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94758abe-bd4b-4bdb-a198-1d22c6ca1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État 0 : [0. 0. 0. 0.]\n",
      "État 1 : [0. 0. 0. 0.]\n",
      "État 2 : [0. 0. 0. 0.]\n",
      "État 3 : [0. 0. 0. 0.]\n",
      "État 4 : [0. 0. 0. 0.]\n",
      "État 5 : [0. 0. 0. 0.]\n",
      "État 6 : [0. 0. 0. 0.]\n",
      "État 7 : [0. 0. 0. 0.]\n",
      "État 8 : [0. 0. 0. 0.]\n",
      "État 9 : [0. 0. 0. 0.]\n",
      "État 10 : [0. 0. 0. 0.]\n",
      "État 11 : [0. 0. 0. 0.]\n",
      "État 12 : [0. 0. 0. 0.]\n",
      "État 13 : [0. 0. 0. 0.]\n",
      "État 14 : [0. 0. 0. 0.]\n",
      "État 15 : [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for state in range(env.observation_space.n):\n",
    "    print(f\"État {state} : {Q_table[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07f555-ad5f-498a-a345-50d40f2212b6",
   "metadata": {},
   "source": [
    "Exercice 3 : Implémentation du Q-Learning \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f45a-c50f-412e-a7bc-f49c6de82991",
   "metadata": {},
   "source": [
    "* **Définition des hyperparamètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef9b686f-c847-439f-aa5d-b7bdd35ddecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Taux d'apprentissage\n",
    "gamma = 0.99 # Facteur de discount\n",
    "epsilon = 1.0 # Exploration initiale\n",
    "epsilon_decay = 0.995 # Décroissance d'epsilon\n",
    "num_episodes = 5000\n",
    "min_epsilon = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cb8fa-1560-4b4f-b71b-e59abb1f949d",
   "metadata": {},
   "source": [
    "* **Mise à jour de la Q-TAble à l'aide de la règle de mise à jour du Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d936d-348c-4477-88ac-6c5d1f279fcf",
   "metadata": {},
   "source": [
    "La Règle de mise à jour du Q-Learning :\n",
    "\n",
    "**Q(s,a) = Q(s,a) + α * [R + γ * max Q(s',a') - Q(s,a)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e432ff3-a06b-420b-b18e-fd92a460cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode 0/5000\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Épisode 1000/5000\n",
      "[[0.29887161 0.19296185 0.16900853 0.13848531]\n",
      " [0.05247418 0.02318166 0.02159192 0.22177339]\n",
      " [0.19084944 0.03954224 0.02218975 0.03285595]\n",
      " [0.01364765 0.02503539 0.00183236 0.00594433]\n",
      " [0.30988251 0.08809798 0.13872619 0.10542775]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23381885 0.03101116 0.06339257 0.00872702]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07279448 0.12700079 0.11377926 0.33720252]\n",
      " [0.06563512 0.37520698 0.12829413 0.06179497]\n",
      " [0.42814026 0.13990428 0.18589879 0.04710909]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02825468 0.0872312  0.01891873 0.29084628]\n",
      " [0.03929817 0.3684789  0.73410175 0.27848993]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Épisode 2000/5000\n",
      "[[0.35830326 0.29548476 0.29115695 0.28237122]\n",
      " [0.05247418 0.02318166 0.02159192 0.29456064]\n",
      " [0.27494061 0.03954224 0.04446804 0.03204886]\n",
      " [0.01364765 0.02253185 0.00183236 0.00594433]\n",
      " [0.37479811 0.21553057 0.22145344 0.22545628]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2607481  0.07028108 0.07723946 0.00872702]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.14713602 0.24983093 0.19393891 0.40337026]\n",
      " [0.12916898 0.44132815 0.19517668 0.09337143]\n",
      " [0.44437897 0.28691965 0.2559658  0.09675798]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09770566 0.20597531 0.16033205 0.46385655]\n",
      " [0.10475974 0.45833228 0.6919179  0.35064093]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Épisode 3000/5000\n",
      "[[0.49796059 0.3771577  0.40469891 0.36636623]\n",
      " [0.07005965 0.05348879 0.02159192 0.40352994]\n",
      " [0.35362508 0.06059453 0.0599298  0.03204886]\n",
      " [0.01364765 0.07070222 0.00183236 0.00594433]\n",
      " [0.5149599  0.31774365 0.27379362 0.24883571]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24527661 0.05692768 0.1393757  0.00872702]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33248226 0.25601707 0.20016128 0.55094284]\n",
      " [0.28583591 0.58129421 0.29346079 0.20686437]\n",
      " [0.4881667  0.32317272 0.30039433 0.12652254]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20909949 0.1420695  0.67247123 0.1458389 ]\n",
      " [0.36231041 0.49991537 0.71919461 0.40167657]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Épisode 4000/5000\n",
      "[[0.55624022 0.46860974 0.47892702 0.46320589]\n",
      " [0.11234659 0.09127449 0.02159192 0.44346809]\n",
      " [0.37256334 0.13984085 0.08948677 0.05972575]\n",
      " [0.01364765 0.09205299 0.00183236 0.00594433]\n",
      " [0.57549339 0.48306367 0.42969453 0.1950072 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27457711 0.09653543 0.13096    0.03386094]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34704851 0.40866141 0.27722542 0.64028463]\n",
      " [0.37339808 0.68315295 0.35315161 0.33652797]\n",
      " [0.65645407 0.27662763 0.34063303 0.15801849]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24152662 0.34784602 0.7389879  0.31505308]\n",
      " [0.52916982 0.8956367  0.62882078 0.51569009]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Épisode 4999/5000\n",
      "[[0.53118815 0.49487622 0.51501373 0.50671856]\n",
      " [0.14971282 0.08214704 0.09557182 0.46364409]\n",
      " [0.39557861 0.13405869 0.09167755 0.12988137]\n",
      " [0.01364765 0.1009643  0.00183236 0.00594433]\n",
      " [0.5400511  0.38157747 0.45033004 0.28979563]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2590253  0.07037433 0.117864   0.09524875]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40241428 0.38837819 0.32574285 0.57191549]\n",
      " [0.37806681 0.61710465 0.32011633 0.41972501]\n",
      " [0.61414479 0.4062194  0.36514374 0.21202895]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2382407  0.5267273  0.6554163  0.38404151]\n",
      " [0.57987965 0.84149308 0.74717006 0.62219945]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Entraînement terminé !\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state, :])  # Exploitation\n",
    "\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Mise à jour de la Q-Table\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Réduction progressive de l'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Affichage de la Q-Table \n",
    "    if episode % 1000 == 0 or episode == num_episodes - 1:\n",
    "        print(f\"\\nÉpisode {episode}/{num_episodes}\")\n",
    "        print(Q_table)\n",
    "\n",
    "print(\"\\nEntraînement terminé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7ca1b-8ba2-44aa-a39d-9279e63b089c",
   "metadata": {},
   "source": [
    "Exercice 4: Evaluaion des performances de l'agent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89e4a1-5931-4515-9909-d8bbf6919439",
   "metadata": {},
   "source": [
    "* **Initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d1adb-91e5-48f7-a5e8-c6b26efdd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeff974-c244-4149-8b9b-905876d6d745",
   "metadata": {},
   "source": [
    "* **Mesure du taux de réussite de l'agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a932e557-af00-412f-9741-2b4e5880894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taux de réussite après entraînement : 83/100 (83.00%)\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q_table[state, :])  # Prendre l'action optimale\n",
    "        new_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        if reward == 1.0:  # L'agent a atteint l'objectif\n",
    "            successes += 1\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nTaux de réussite après entraînement : {successes}/{num_test_episodes} ({(successes/num_test_episodes) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a02e2-67b7-4028-98e6-80ddf00e744b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f1fb0-e53f-4f2b-abde-0e35c20a79ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
